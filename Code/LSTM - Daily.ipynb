{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM - Daily.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"Rt4tUPAc1gbj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7F89sA81gbn","colab_type":"code","colab":{}},"source":["\n","import time\n","import math\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","\n","from keras.layers.core import Dense, Activation, Dropout\n","from keras.layers.recurrent import LSTM\n","from keras.models import Sequential\n","\n","# We want to be able to repeat the experiment so we fix to a random seed\n","np.random.seed(70)\n","\n","# Let us loading data \n","data=pd.read_csv(\"ASX200Daily.csv\",usecols=['Close'])##usecols=['Date', 'Close'])\n","data = data.dropna()  # Drop all Nans\n","data = data.values  # Convert from DataFrame to Python Array  \n","                    # You need to make sure the data is type of float\n","                    # you may use data = data.astype('float32') if your data are integers\n","\n","# Prepare data .....\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XtEkhWC71gbq","colab_type":"code","colab":{}},"source":["\n","\"\"\" Scaling ...\n","Neural networks normally work well with scaled data, especially when we use\n","the sigmoid or tanh activation function. It is a good practice to scale the\n","data to the range of 0-to-1. This can be easily done by using scikit-learn's \n","MinMaxScaler \n","\"\"\"\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","data = scaler.fit_transform(data)\n","\n","\"\"\"  Splitting ...\n","We are going to use a time window of k = 3, so we will split the time series as\n","   x_1, x_2, x_3,     x_4  [prediction]\n","   x_2, x_3, x_4,     x_5\n","   x_3, x_4, x_5,     x_6 \n","   ....\n","\"\"\" \n","train_size = int(len(data)*0.7)   \n","test_size = len(data) - train_size \n","Dtrain, Dtest = data[0:train_size,:], data[train_size:len(data),:]\n","\n","time_window =15\n","Xtrain, Ytrain = [], []\n","for i in range(len(Dtrain) - time_window -1):\n","    Xtrain.append(Dtrain[i:(i+time_window), 0])   \n","    Ytrain.append(Dtrain[i+time_window, 0])      \n","Xtrain = np.array(Xtrain)      \n","Ytrain = np.array(Ytrain) \n"," \n","\n","Xtest, Ytest = [], []\n","for i in range(len(Dtest) - time_window -1):\n","    Xtest.append(Dtest[i:(i+time_window), 0])   \n","    Ytest.append(Dtest[i+time_window, 0])       \n","Xtest = np.array(Xtest)    \n","Ytest = np.array(Ytest) \n","\n","\n","Xtrain = np.reshape(Xtrain, (Xtrain.shape[0], time_window, 1))  \n","Xtest = np.reshape(Xtest, (Xtest.shape[0], time_window, 1))  \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NGINpzSR1gbs","colab_type":"code","colab":{},"outputId":"18e49ad4-c840-488a-af78-cc4564abf5f4"},"source":["# Define our model .....\n","model = Sequential()\n","\n","\n","MyBatchSize = 1\n","\n","model.add(LSTM(100, input_shape = (time_window,1), batch_size=MyBatchSize,\n","        return_sequences=False))   # Many-to-One model\n","\n","# It seems without using batch_size = 1 here causes some problems, a bug?\n","\n","model.add(Dropout(0.2))            # Impose sparsity as we have so many hidden neurons\n","# As we will have 100 outputs from LSTM at each time step, we will use a linear \n","# layer to map them to a single \"prediction\" output\n","model.add(Dense(\n","        output_dim=1))\n","model.add(Activation(\"linear\"))\n","\n","# Compiling model for use\n","start = time.time()\n","model.compile(loss=\"mse\", optimizer=\"Adadelta\")\n","print(\"Compilation Time : \", time.time() - start)\n","\n","\n","# Training\n","model.fit(\n","\t    Xtrain,\n","\t    Ytrain,\n","\t    batch_size=MyBatchSize,\n","\t    nb_epoch=50,   # You increase this number\n","\t    validation_split=0.05)\n","\n","# Predicting\n","# make predictions\n","trainPredict = model.predict(Xtrain,batch_size=MyBatchSize)\n","testPredict = model.predict(Xtest,batch_size=MyBatchSize)\n","# invert predictions due to scaling\n","trainPredict = scaler.inverse_transform(trainPredict)\n","Ytrain = scaler.inverse_transform(Ytrain[:,np.newaxis])\n","testPredict = scaler.inverse_transform(testPredict)\n","Ytest = scaler.inverse_transform(Ytest[:,np.newaxis])\n","# calculate root mean squared error\n","trainScore = math.sqrt(mean_squared_error(Ytrain, trainPredict[:,0]))\n","print('Train Score: %.2f RMSE' % (trainScore))\n","testScore = math.sqrt(mean_squared_error(Ytest, testPredict[:,0]))\n","print('Test Score: %.2f RMSE' % (testScore))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/Users/ratnesh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1)`\n","  app.launch_new_instance()\n","/Users/ratnesh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Compilation Time :  0.03567790985107422\n","Train on 3189 samples, validate on 168 samples\n","Epoch 1/50\n"," 453/3189 [===>..........................] - ETA: 46s - loss: 0.0055"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-494c82507174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMyBatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# You increase this number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \t    validation_split=0.05)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Predicting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"bBiyvw1F1gby","colab_type":"code","colab":{}},"source":["# Plotting results\n","# shift train predictions for plotting\n","trainPredictPlot = np.empty_like(data)\n","trainPredictPlot[:, :] = np.nan\n","trainPredictPlot[time_window:len(trainPredict)+time_window, :] = trainPredict\n","# shift test predictions for plotting\n","testPredictPlot = np.empty_like(data)\n","testPredictPlot[:, :] = np.nan\n","testPredictPlot[len(trainPredict)+(time_window*2)+1:len(data)-1, :] = testPredict\n","# plot baseline and predictions\n","plt.plot(scaler.inverse_transform(data), label='True Data')\n","plt.plot(trainPredictPlot, label='Train Prediction')\n","plt.plot(testPredictPlot, label='Test Prediction')\n","plt.legend()\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLbfIICB1gb0","colab_type":"code","colab":{}},"source":["## Overall Predictions \n","Xall = [] \n","predictions = [] \n","\n","for i in range(6):\n","    Xall = []\n","    for j in range(len(data) - time_window):\n","        Xall.append(data[j:(j+time_window), 0])   # pick up the section in time_window size\n","    Xall = np.array(Xall)    # Convert them from list to array  \n","    Xall = np.reshape(Xall, (Xall.shape[0], time_window, 1)) \n","    \n","    allPredict = model.predict(Xall,batch_size=MyBatchSize)\n","    data=np.append(data,allPredict[-1]) \n","    data=data[:,np.newaxis]  \n","    allPredict = scaler.inverse_transform(allPredict)\n","    predictions.append(allPredict[-1])\n","    \n","\n","data=data[:len(data)-1]\n","\n","allPredictPlot = np.empty_like(data)\n","allPredictPlot[:, :] = np.nan\n","allPredictPlot[time_window:, :] = allPredict\n","\n","plt.figure()\n","plt.plot(scaler.inverse_transform(data), label='True Data')\n","plt.plot(allPredictPlot, label='One-Step Prediction') \n","plt.legend()\n","plt.show()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYwKhWvP1gb3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}